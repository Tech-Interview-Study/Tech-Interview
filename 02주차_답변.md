# 📄Table of Contents

# 기술
## Statistics/Math
- [공분산과 상관계수는 무엇일까요?](#1)
- [조건부 확률은 무엇일까요?](#2)
- [Missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?](#3)


## Machine Learning
- [차원의 저주에 대해 설명하고, 이를 해결하기 위한 방법을 말해주세요.](#4)
- [L1, L2 정규화에 대해 설명해주세요.](#5)
- [Cross Validation은 무엇이고 어떻게 해야하나요? (각 종류와 장단점)](#6)
- [KNN 과 K-means 에 대해서 설명해주세요.](#7)
- [XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?](#8)
- [회귀 / 분류시 알맞은 metric은 무엇인가요?](#9)


## Deep Learning
- [오버피팅일 경우 어떻게 대처해야 할까요? 알고계신 방법들을 전부 말해주세요.](#10)
- [Dropout은 무엇이며 왜 사용하는 지 말해주세요.](#11)
- [GD가 Local Minima 문제를 피하는 방법을 말해주세요.](#12)
- [Batch Normalization은 무엇인가요?](#13)
- [BN 적용해서 학습 이후 실제 사용시에 주의할 점은 무엇인가요(코드적인 부분 포함)?](#14)

## Database
- [NoSQL과 RDBMS의 차이점을 설명해주세요.](#15)


</br>

# 인성
- [본인이 생각하는 소통의 정의를 내려주세요.](#16)
- [본인만의 차별화된 강점은 무엇인가요?](#17)
- [팀으로 일하는 것을 선호하시나요? 개인으로 일하는 것을 선호하시나요?](#18)
- [동료간의 갈등을 겪은 경험이 있나요? 있다면 구체적으로 얘기해주세요.](#19)
- [자신을 채용해야 하는 이유는 뭐라고 생각하시나요?](#20)
- [본인의 장점, 단점을 말해주세요](#21)
---

## #1
### 공분산과 상관계수는 무엇일까요?

공분산과 상관계수는 **두 변수 간의 관련성**을 측정하는 데 사용되는 통계적 개념입니다. **공분산**은 두 변수의 변화량이 함께 얼마나 많이 변하는지를 나타내는 값으로,  변수 간에 **양의 상관관계가 있는지, 음의 상관관계**가 있는지를 알 수 있습니다. **상관계수**는 공분산을 **각 변수의 표준편차로 나누어, 각 변수의 척도에 대한 영향을 제거**한 값입니다. **1과 1 사이의 값**을 가지며, 1은 완전한 양의 상관관계, -1은 완전한 음의 상관관계를 나타내며, 0은 상관관계가 없음을 나타냅니다.

</br>

## #2
### 조건부 확률은 무엇일까요?

**조건부 확률**은 **하나의 사건이 일어났을 때, 다른 사건이 일어날 확률**입니다.

> 사건 A가 일어났다는 전제 하에 사건 B가 일어날 확률은 사건 A와 B가 동시에 일어날 확률에 사건 A가 일어날 확률을 나누어 준 값입니다.

</br>

## #3
### Missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?

결측값이 많지 않은 경우에는 제거해주면 되지만, 결측값이 많거나, 데이터가 적은 경우에는 이를 **적절한 값으로 채워 가능한 데이터의 양을 확보**해야 합니다. 이를 통해 모델이 보다 정확한 예측을 하도록 할 수 있습니다. 결측값은 **0이나 평균, 중앙값, 최빈값** 등으로 대체할 수 있으며, **회귀 분석, K-NN 등의 머신러닝 알고리즘을 사용**하여 예측값으로 대체하는 방법이 있습니다.


> 이를 해결하기 위한 가장 쉬운 방법은 누락된 데이터를 제거하는 것입니다. 하지만 중요한 정보를 가진 데이터를 잃을 위험을 가지고 있습니다. 또 한 컬럼에 있는 missing value를 결측 되지 않은 다른 값의 평균이나 중앙값으로 대체하는 것입니다. 이 방법은 쉽고 작은 빠르다는 장점이 있지만 다른 feature 간의 상관관계가 고려되지 않고 인코딩 된 범주형 feature에 대해 안 좋은 결과를 제공한다는 단점을 가지고 있습니다. 최빈값, 0, 임의 값으로 채워넣을 수도 있습니다. 이 방식은 쉽고 범주형 feature에 잘 동작한다는 장점이 있지만 이것 또한 다른 feature 간의 상관관계가 고려되지 않고 데이터에 bias를 만든다는 단점이 있습니다.  
머신러닝을 통해 해결하는 방법으로는 KNN으로 가까운 이웃간의 거리기반 분석을 통해 채워넣을 수 있습니다. mean, median이나 most fequent보다 정확할 때가 많지만 메모리가 많이 필요한 방법입니다.  
Pandas에서는 dropna, fillna를 이용해 누락된 데이터를 제거하거나 채울 수 있습니다.  

</br>

## #4
### 차원의 저주에 대해 설명하고, 이를 해결하기 위한 방법을 말해주세요.

**Feature에 비해 데이터가 너무 적어서 생기는 현상**으로 과적합의 발생 가능성이 증가하게 됩니다. 이를 해결하기 위해서는 더 많은 데이터를 추가하거나 PCA와 같이 피처를 함축적으로 잘 설명할 수 있도록 저차원으로 매핑하는 차원축소를 하거나, 중요한 변수만 선택하여 차원을 줄이는 방법이 있습니다.

> 이를 해결하기 위해 가장 대표적인 방법은 **Feature Selection**과 **Feature Extraction**이 있습니다. 우선 **Feature Selection**은 우리가 예측하고자 하는 타겟 변수와 관련이 높은 변수들을 추려내는 방법입니다. 이 방법을 이용해 불필요한 변수들을 처냄으로써 모델의 학습 시간을 줄이고 성능 또한 높일 수 있습니다. **Feature Extraction**은 데이터 특성을 가장 잘 표현하는 **주성분**을 추출해 데이터 양을 줄이는 방법이며, 이에 사용되는 대표적인 방법은 바로 **주성분 분석(PCA)** 입니다.

</br>

## #5
### L1, L2 정규화에 대해 설명해주세요.

L1 정규화와 L2 정규화는 가중치(w)에 대한 제약 조건을 추가하여 모델의 과적합을 방지하고 일반화 성능을 향상시키는 방법입니다. 즉, 가중치의 크기를 제한함으로써 모델의 복잡도를 감소시켜 새로운 데이터에 대해 더 일반화된 예측을 하도록 하는 것입니다.  
L1 정규화는 **가중치 값들의 절댓값의 합을 최소화**하여, 불필요한 가중치 값들을 0으로 만들어 모델에서 특정한 입력에만 반응하는 가중치 값들을 제거할 수 있습니다. L2정규화는 **가중치 값들의 제곱합을 최소화**하여 가중치 값들 간의 차이를 줄여줄 수 있습니다.

</br>

## #6
### Cross Validation은 무엇이고 어떻게 해야하나요? (각 종류와 장단점)

Cross Validation은 test 데이터에 overfitting을 방지하기 위해, test와 분리한 train 데이터를 다시 **train-validation 세트로 나누어 실험**을 해 그 평균을 검증값으로 이용하는 것을 말합니다.  
Cross Validation의 종류로는 n개의 data를 균등하게 섞고 k개의 그룹으로 나눠서 1개만 test set, k-1개는 train set으로 사용하는 K-fold CV, 1개의 data sample만 test set, n-1개는 train set으로 사용하는 LOOCV, training set에도 cross validation을 다시 적용해서 best parameter를 찾아 training set에 적용해 train하는 Nested CV 등이 있습니다.  
계층별 k-겹 교차검증(Stratified K-Fold Cross Validation)은 원본 데이터의 레이블 분포를 먼저 고려하여, 이 분포와 동일하게 학습과 검증 데이터 세트를 분할하는 방식입니다.
Group K-Fold Cross Validation은 데이터를 그룹으로 나누어, 특정 그룹에 속한 모든 데이터를 한 폴드의 검증 데이터셋으로 사용하는 방식입니다.

</br>

## #7
### KNN 과 K-means 에 대해서 설명해주세요.

KNN은 지도학습으로, 주변의 **가장 가까운 K개의 데이터를 보고 데이터가 속할 그룹을 판단**하는 classification 알고리즘입니다.  
K-means는 비지도학습으로, **별도의 레이블이 없는 데이터 안에서 데이터 간 유사도를 기준으로 묶는** clustering 알고리즘입니다.

> KNN과 K-means의 가장 큰 차이는 지도학습과 비지도학습입니다. 둘은 모두 K개의 점을 지정하여 거리르 기반으로 구현되는 거리기반 분석 알고리즘입니다.  
먼저 **KNN**(최근접 이웃방법)은 해당 데이터와 가장 가까이 있는 K개의 데이터를 확인하여 새로운 데이터 특성을 확인하는 방법입니다. 지도학습 알고리즘 중 하나로 K가 짝수면 1:1 대응이 될 수도 있기 때문에 K는 홀수를 쓰는 것이 보편적입니다. KNN은 **회귀와 분류 모두 사용 가능**합니다.  
**K-means** 알고리즘은 데이터를 K개의 군집(cluster)으로 묶는 clustering 알고리즘입니다. 여기서 K는 묶을 그룹의 수를 말하고 Means는 데이터로부터 그 데이터가 속한 그룹의 중심까지의 평균 거리를 의미하며 이 값을 최소화 하는 것이 K-means입니다.

</br>

## #8
### XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?

XGBoost는 **Gradient boosting**을 이용한 모델입니다. 병렬 연산을 지원하고 GPU를 사용할 수 있는 옵션이 있기 때문에 학습하는데 소요되는 시간을 절약할 수 있습니다.  
XGBoost는 **빠르고** 다양한 커스텀 옵션으로 **유연성**이 좋습니다. 또한 그리디 알고리즘으로 과적합 방지가 가능해 캐글에서 좋은 성능을 보이고 있습니다.

> **XGBoost**는 속도가 빠르고, 자원 효율성이 높아서 캐글에도 인기가 많습니다. **XGBoost**는 기존 Gradient Tree Boosting 알고리즘에 과적합 방지를 위한 기법이 추가된 지도학습 알고리즘 입니다. **XGBoost**는 기본 학습기를 의사결정나무로 하며 Gradient Boosting과 같이 **Gradient(잔차)를 이용**하여 이전 모델의 약점을 보완하는 방식으로 학습니다. **XGBoost**는 과적합 방지가 잘되고 예측 성능이 좋다는 장점이 있지만 작은 데이터에 대해서는 과적합 가능성이 있다는 단점을 가지고 있습니다.
분류와 회귀문제에 모두 사용가능하며, XGBoost는 자체에 얼리스탑과 같은 과적합 규제 기능이 있어 강한 내구성을 지닙니다. 또한, 다양한 파라미터 옵션을 제공하는 Customizing이 용이하기도 합니다. 이러한 이유들로 기존 GBoost보다 XGBoost가 더 인기 있습니다.

</br>

## #9
### 회귀 / 분류시 알맞은 metric은 무엇인가요?

회귀 문제에서는 실제 값과 모델이 예측하는 값의 차이에 기반을 둔 metric(평가)을 사용합니다. 대표적으로 **RSS(단순 오차 제곱 합), MSE(평균 제곱 오차), MAE(평균 절대값 오차)** 가 있습니다.  
분류 문제에서는 분류 결과의 신뢰도를 나타낼 수 있는 metric을 사용합니다. 대표적으로는 **Accuracy, Precision, Recall와 이를 조합합 F1 score**등이 있습니다.

> [Why is cross entropy not a common evaluation metric for model performance?](https://stats.stackexchange.com/questions/370861/why-is-cross-entropy-not-a-common-evaluation-metric-for-model-performance)

</br>

## #10
### 오버피팅일 경우 어떻게 대처해야 할까요? 알고계신 방법들을 전부 말해주세요.

Overfitting이 일어날 경우에는 augmentation를 이용해 **데이터의 절대적인 양을 늘리거나 regularization, dropout을 사용하고, early stopping**으로 일정 횟수 이상 validation loss가 증가하는 시점부터 overfitting이 발생했다고 판단하고 이에 학습을 종료시킵니다. 또는 **Batch Normalization**으로 데이터 분포를 통일 시켜줘서 과적합을 방지합니다.

> Overfitting을 막기 위해 여러 방법이 있습니다. 
> 1. 데이터의 양이 적을 경우, 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하게 되므로 과적합 현상이 발생할 확률이 늘어납니다. 이럴 경우 기존의 데이터를 증강시켜 데이터를 늘리고 데이터가 많을수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있습니다.
> 2. 복잡한 모델은 간단한 모델보다 과적합될 가능성이 높으므로 정규화를 통해서 복잡한 모델를 좀 더 간단하게 하는 방법이 있습니다.
> 3. Dropout을 사용해 학습과정마다 일정 비율의 뉴런만 사용하는 방법을 사용합니다.
> 4. 일정 횟수 이상 validation loss가 증가하는 시점부터 overfitting이 발생했다고 판단하고 이에 학습을 종료시킵니다,
> 5. Batch Normalization으로 데이터 분포를 통일 시켜줘서 과적합을 방지합니다. 

</br>

## #11
### Dropout은 무엇이며 왜 사용하는 지 말해주세요.

Dropout은 서로 연결된 layer에서 **0에서 1사이의 확률로 뉴런을 제거(drop)** 하는 기법입니다. 꺼지는 뉴런의 종류와 개수는 오로지 랜덤하게 Dropout rate에 따라 결정이됩니다.
Dropout은 어떤 특정한 설명변수 Feature만을 과도하게 집중하여 학습함으로써 발생할 수 있는 overfitting을 방지하기 위해 사용됩니다. 학습 데이터에 의해 각 node들이 **co-adaptation**되는 현상을 방지하고 매 학습마다 drop 되는 뉴런이 달라지기 때문에 서로 다른 모델들을 **앙상블 하는 것과 같은 효과**가 있습니다.

> co-adaptation: 어느 시점에서 같은 층의 두 개 이상의 노드의 입력 및 출력 연결강도가 같아지면, 아무리 학습이 진행되어도 그 노드들은 같은 일을 수행하게 되어 불필요한 중복이 생기는 문제

</br>

## #12
### GD가 Local Minima 문제를 피하는 방법을 말해주세요.

초기값을 잘 선정하거나, LambdaLR, CosineAnnealingLR 등 **학습률을 조절하는 학습률 스케줄링(learning rate scheduling)** 기법을 사용할 수 있습니다. 또는 Momentum을 사용하여 이전에 이동했던 방향과 크기를 고려하여 local minima에 빠지는 것을 방지할 수 있습니다. 이 외도 Gradient descent에서 파생된 Adagrad, Adadelta, RMSprop, Adam 등 다른 optimizer를 사용하는 방법이 있습니다.

</br>

## #13
### Batch Normalization은 무엇인가요?

학습 과정에서 **Mini-batch마다 평균과 분산을 활용하여 데이터의 분포를 정규화**를 하고, scale factor와 shift factor를 이용하여 새로운 값을 생성하는 것입니다. 이때 scale factor와 shift factor는 다른 레이어에서 weight를 학습하듯이 역전파에서 학습됩니다.
기존에는 learning rate를 높게 잡을 경우 gradient가 explode/vanish 하거나, local minima에 빠지는 문제가 있었습니다. 그러나 Batch Normalization을 사용할 경우 **parameter의 scale에 영향을 받지 않게 되어, learning rate를 크게 잡을 수 있어 안정적으로 빠른 학습**을 할 수 있습니다.   
여기서 중요한 것은 Batch Normalization은 학습 단계와 추론 단계에서 조금 다르게 적용되어야 합니다.

</br>

## #14
### BN 적용해서 학습 이후 실제 사용시에 주의할 점은 무엇인가요(코드적인 부분 포함)?

Inference 시 input을 이용해 BN을 하면 모델이 train에서 input의 분포를 추정한 의미가 없어지기 때문에, inference 시에는 결과를 deterministic하게 만들기 위해 **미리 저장해둔 mini-batch의 moving average를 이용**해 정규화를 합니다.  
Batch normalization을 적용할 때는 **activation function 앞에 적용**해야 합니다. 일반적으로 Hidden Layer - Batch Normalization - Activiation Function 순서로 적용합니다. Batch Normalization 적용 후 ReLU와 같은 활성화 함수를 적용하면 데이터의 절반가량이 음수이기에 의미 없는 값을 가지게 될 수 있습니다.  

Pytorch에서 BatchNormalization을 사용하는 대표적인 방법은 torch.nn.BatchNorm1d와 torch.nn.BatchNorm2d를 사용하는 것입니다.  
이 때, 차이점은 BatchNorm1d의 경우 Input과 Output이 (N, C) 또는 (N, C, L)의 형태를 가지고 BatchNorm2d의 경우 Input과 Output이 (N, C, H, W)의 형태를 가집니다. 여기서 N은 Batch의 크기를 말하고 C는 Channel을 말합니다. BatchNorm1d에서의 L은 Length을 뜻하고 BatchNorm2d에서의 H와 W 각각은 height와 width를 뜻합니다.
Inference 시에는 BATCHNORM함수에 track_running_stats라는 파라미터에 False의 값을 주어야 합니다.

</br>

## #15
### NoSQL과 RDBMS의 차이점을 설명해주세요.

**RDBMS**는 관계형 데이터베이스 관리 시스템을 의미합니다. 다른 테이블과 관계를 맺고 모여있는 집합체로 이해할 수 있습니다. 이러한 관계를 나타내기 위해 **외래 키(foreign key)** 를 사용한 테이블 간 Join이 가능하다는게 RDBMS의 가장 큰 특징입니다. 정해진 스키마에 따라 데이터를 저장하여야 하므로 **정확한 데이터 구조를 보장**하는 장점이 있습니다. 반면에 테이블간 관계를 맺고 있어 시스템이 커질 경우 JOIN문이 많은 복잡한 쿼리가 만들어질 수 있고 스키마로 인해 데이터가 유연하지 못해 나중에 스키마가 변경 될 경우 번거롭고 어렵다는 단점이 있습니다.  
**NoSQL**은 RDBMS와는 달리 테이블 간 관계를 정의하지 않습니다. 데이터 테이블은 그냥 하나의 테이블이며 따라서 일반적으로 테이블 간 Join도 불가능합니다. 빅테이터의 등장으로 인해 데이터와 트래픽이 기하급수적으로 증가함에 따라 RDBMS에 단점인 성능을 향상시키기 위해 등장했고 데이터 일관성은 포기하되 비용을 고려하여 여러 대의 데이터에 분산하여 저장하는 Scale-Out을 목표로 등장했습니다. **스키마가 없기 때문에 유연하며 자유로운 데이터 구조**를 가질 수 있고 언지든 저장된 데이터를 조정하고 새로운 필드를 추가할 수 있다는 장점이 있습니다. 반면에, 스키마가 존재하지 않기에 명확한 데이터 구조를 보장하지 않으며 데이터 구조 결정하기가 어려울 수 있다는 단점이 있습니다.

----
# 인성

## #16
### 본인이 생각하는 소통의 정의를 내려주세요.

- 소통은 다름 사람의 의견을 경청하고, 이에 대한 본인의 의견을 공유하는 것이라고 생각합니다. 이 과정에서 서로 다른 부분을 조율해 나가며, 보다 나은 방향을 향해 갈 수 있게 된다고 생각합니다.

- 제가 생각하는 소통이란 타인의 의견을 경청하고 이해할 뿐만 아니라 제 의견을 명료하게 정리해 전달해 서로 같은 시야를 공유할 수 있는 대화라고 생각합니다.

- 소통이란 자신의 생각을 잘 전달하고 상대방은 잘 들어주며 의사의 전달과 상호교류가 이뤄지는 것이라고 생각합니다.

</br>

## #17
### 본인만의 차별화된 강점은 무엇인가요?

- 저만의 차별화된 강점은 사교성이라고 생각합니다. 저는 다년간의 동아리 활동을 통해 다양한 배경을 가진 사람들과 협력하고 의사소통하는 방법을 익혔습니다. 사교성은 제가 프로젝트나 경진대회에 참가해 ~하는 데에도 좋은 영향을 줬다~(직무 연관시켜서 문장 마무리)

- 저는 소통을 잘하는 사람입니다. 그래서 제 주변 사람들은 저와 무언가를 할때 저와 함께 하는 것을 좋아하고 실제 소통에 대한 좋은 평을 많이 받았습니다. 이런 차별된 강점을 통해 팀원과 소통을 원활하게 하였고 코드 개선이라는 좋은 결과까지 얻었습니다. 소통을 통해 더 좋은 결과를 만들 수 있는 것이 제가 가진 차별된 강점입니다.

- 명확한 커뮤니케이션과 목표에 대한 열정으로 주도적인 업무처리를 하는 것이라고 생각합니다. 명확한 커뮤니케이션의 경우, 학부 연구실에서의 발표와 개발 스터디 동아리에서의 멘토링을 통해 명확하게 정보를 전달하는 역량을 기를 수 있었습니다. 실제로 부스트캠프의 동료 피드백에서 명확한 커뮤니케이션과 열정과 솔선수범에 대한 키워드에도 높은 점수를 받았었습니다.

</br>

## #18
### 팀으로 일하는 것을 선호하시나요? 개인으로 일하는 것을 선호하시나요?

- 저는 팀으로 일하는 것을 선호합니다. 팀으로 일을 하면 구성원의 적성에 맞게 효율적으로 업무를 분담할 수 있고, 혼자 일할땐 미처 생각하지 못한 부분이나 놓친 부분을 동료가 점검해줄 수 있기 때문입니다.  
\+ 부캠 하면서 경험했던 구체적인 일화 추가하면 좋을듯

- 팀으로 일하는 것을 선호합니다. 개인보다는 팀으로 일을 하면, 각자 역할에 대한 책임감이 생기며, 이에 일의 효율성이 올라간다고 생각합니다. 또한 개인의 부족한 점을 다른 팀원을 통해 메우며, 빠르게 성장할 수 있다고 생각합니다. 


- 저는 팀으로 하는 것을 선호합니다. 혼자 할 수 있는 역량보다 같이 했을 때 더 좋은 결과를 낼 수 있다고 생각하기 때문입니다. 

</br>

## #19
### 동료간의 갈등을 겪은 경험이 있나요? 있다면 구체적으로 얘기해주세요.

- 개발 이외의 경험도 괜찮은지? 그렇다면 동아리 회장으로 동아리 운영하면서 임원들과 갈등 + 해결한 경험 녹여내기.   

- 갈등이 어떻게 시작되었는지 간단히 설명 후 갈등을 해결하기 위해 어떻게 노력했는지 구체적으로 답변
- 입사후 비슷한 문제가 발생할 경우 어떻게 대처할 것인지

- 팀 프로젝트에서 팀원들 간의 커뮤니케이션이 안 맞던 팀원이 있었습니다. => 구체적으로  
그래서 팀 프로젝트를 하는 동안, 중간에 다른 팀원의 의견을 정리해서 이를 원활하게 전달한 경험이 있습니다. 이 경험을 통해 협업을 할 때, 팀원간의 소통의 중요성을 깨닫게 되었습니다.

</br>

## #20
### 자신을 채용해야 하는 이유는 뭐라고 생각하시나요?
(이 질문 받는게 기회일 수 있다..!)

- ai 개발자로서의 기초 지식 + 팀워크 경험? --> 성장 가능성이 있음을 어필

- 강점 + 자신감 + 열정적 + 차별성 + 회사에 줄 이익

- 익숙하지 않은 기술도 빠르게 습득하는 편이라고 생각합니다. 지금까지 진행해 온 프로젝트에서 새로운 기술을 사용하여 진행한 경우가 있었는데, 그때마다 빠르게 습득하여 예정 기한 내로 기대했던 결과물을 도출할 수 있었습니다. 따라서 빠르게 성장하여 필요한 역량을 길러갈 수 있는 잠재력이 있다고 생각합니다.

</br>

## #21
### 본인의 장점, 단점을 말해주세요

- 제 장점은 사교성이라고 생각합니다. 저는 다년간의 동아리 활동을 통해 다양한 배경을 가진 사람들과 협력하고 의사소통하는 방법을 익혔습니다.  
제 단점은 즉흥성입니다. 저는 일을 차례대로 처리하지 못하고 그때그때 생각난, 하고싶은 일을 먼저 처리하는 경향이 있어 일을 벌리기만 하고 제대로 끝내지 못하는 버릇이 있었습니다. 이를 고치기 위해 저는 투두리스트를 활용해 매주, 매일 끝내야 할 일을 정리해두고 리스트에 맞춰 실행에 옮기고 있습니다.

- 장점 -> 차별화된 강점 동일  
나의 단점은 뭐다 -> 정확히 인지하고 있다 -> 어떤 노력을 하고 있다 -> 개선이 되고 있다는 얘기를 듣는다.


- 저의 장점은 목표한 일에 대한 열정이라고 생각합니다. 이에 일과 관련된 부분에서는 빠른 아웃풋을 낼 수 있습니다. 반면 단점은 목표한 일과 관련되지 않은 것들에 관심이 없다는 점입니다. ⇒ 어떤 노력을 하며 고치고 있는지 ..



