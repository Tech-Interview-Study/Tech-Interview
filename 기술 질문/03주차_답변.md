# 📄Table of Contents

# 기술
## Statistics/Math
- [엔트로피(Entropy)에 대해 설명해주세요. 가능하면 정보이득(Information Gain)도요.](#1)
- [로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.](#2)
- [베이즈 정리에 대해 설명해주세요.](#3)
- [신뢰 구간(Confidence Interval;CI)의 정의는 무엇인가요?](#4)
- [평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?](#5)
- [필요한 표본의 크기를 어떻게 계산합니까?](#6)

## Machine Learning
- [SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요?](#7)
- [ROC 커브에 대해 설명해주실 수 있으신가요?](#8)
- [회귀 / 분류시 알맞은 손실함수와 이에 대한 설명](#9)
- [최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?](#10)
- [머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?](#11)

## Deep Learning
- [Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?](#12)
- [알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)](#13)
- [효율적인 GPU의 사용을 위해 어떻게 dataloader를 조절할 수 있는가?](#14)
- [하이퍼 파라미터는 무엇인가요?](#15)
- [미니배치를 작게 할때의 장단점은?](#16)

## Operating System
- [캐시의 지역성에 대해 설명해주세요.](#17)

</br>

# 인성
- [비 IT 동료와 효과적으로 의사소통하려면 어떻게 할지 말해주세요?](#18)
- [이상적인 개발 환경이란 무엇이라고 생각하시나요?](#19)
- [상사와 의견차가 좁혀지지 않는 갈등이 일어났을 때 어떻게 해결할 것인가?](#20)
- [어떤 개발자가 잘하는 개발자라고 생각하는가?](#21)
- [간단한 1분 자기소개 부탁드려요](#22)
- [최근 관심있는 분야가 있으신가요?](#23)

---

</br>

## #1
###  엔트로피(Entropy)에 대해 설명해주세요. 가능하면 정보이득(Information Gain)도요.

엔트로피는 정보 이론에서 사용되는 개념으로 1로 갈수록 불순하고, 0으로 갈수록 불순하지 않다는 의미입니다. 어떤 시스템의 **불확실성** 혹은 **무질서도**를 나타내는 물리량입니다. 따라서 각 클래스에 대한 엔트로피가 낮을수록 해당 클래스의 예측이 확실하다는 것을 의미합니다.  
정보 이득은 어떤 속성을 기준으로 데이터를 분할할 때, 분할 이전과 이후의 엔트로피 차이로 **해당 속성이 얼만큼의 정보를 제공하는지**를 나타내는 개념입니다. 

</br>

## #2
### 로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.

취하는 경우는 **정규성**을 높이고 분석시에 정확한 값을 얻기 위함입니다. 단위수가 너무 큰 값들을 바로 회귀분석 할 경우, 결과를 왜곡할 우려가 있으므로 이를 방지하기 위해 로그함수를 취해줍니다. 또한 로그함수를 취함으로써 **비선형관계의 데이터를 선형으로** 만들 수 있습니다. 이때 로그 함수는 0~1 사이에서는 음수값을 가지므로 log(1+x)와 같은 방법으로 처리해주어야 합니다.

> 사례) 연령 같은 경우에는 숫자의 범위가 약 0세~120세 이하지만, 재산 보유액 같은 경우에는 0원에서 몇 조까지 올라갈 수 있습니다. 이럴 경우 데이터 간 단위가 달라지면 결과값이 이상해 질 수 있기 때문에 log를 이용해 큰 수를 같은 비율의 작은 수로 바꿔줍니다.

</br>

## #3
### 베이즈 정리에 대해 설명해주세요.

현재 주어진 **모수(가정)** 에서 이 데이터가 관찰될 가능성인 **가능도(Likelihood)와** 데이터 전체의 분포인 **증거(Evidence)를 바탕으로** 가설에 대해 사전에 세운 확률인 **사전확률을** 실제로 가설이 성립할 확률인 **사후확률로 업데이트**하는 것입니다. 즉, 조건부 확률에 사전확률(prior)을 활용하여 통계적 추론을 하는 방법입니다.  
따라서 데이터가 주어지기 전에 이미 어느 정도 확률값을 예측하고 있을 때 이를 새로 수집한 데이터와 합쳐서 최종 결과에 반영할 수 있습니다.

</br>

## #4
### 신뢰 구간(Confidence Interval;CI)의 정의는 무엇인가요?

신뢰구간은 **모수가 실제로 포함될 것으로 예측되는 범위**입니다. 집단 전체를 연구하는 것을 불가능하므로, 샘플링된 데이터를 기반으로 모수의 범위를 추정하기 위해 사용됩니다. 따라서, 신뢰 구간은 샘플링된 표본이 연구중인 모집단을 얼마나 잘 대표하는지 측정하는 방법입니다. 일반적으로 95% 신뢰수준이 사용됩니다.

</br>

## #5
### 평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?

 대부분의 **데이터가 몰려있는 상황에서는 평균을, 이상치가 많이 있는 상황에는 중앙값을 사용**하는 것이 좋습니다. 평균은 모든 관측값을 반영하기 때문에 극단적인 이상치가 있는 경우, 이에 영향을 받을 수 있습니다. 따라서, 이러한 경우에는 데이터들의 가운데에 위치한 중앙값을 사용하는 것이 해당 집단을 대표하는 값이 될 수 있습니다.

</br>

## #6
### 필요한 표본의 크기를 어떻게 계산합니까?

![](https://user-images.githubusercontent.com/90206705/232524877-537946b1-2acd-4423-be72-4df05cf43c8d.png)

**모집단의 크기 N을 구하고, 신뢰수준 z와 오차범위 e를 선정**하여 표본의 크기를 구할수 있습니다. 이때, 오차범위는 작을 수록 모집단의 특성에 대한 유용한 정보를 제공하지만 모집단에 대한 추론이 틀릴 가능성도 높아지므로 10% 를 넘지 않게 해야 합니다. 일반적으로 신뢰도는 90%, 95%, 99%를, 표준편차는 0.5를 사용합니다. 이때 주의 할 점은 오차 한계를 더 작게 하려면 동일한 모집단에서 표본 크기가 더 커야 하고 더 높은 표집 신뢰 수준을 원한다면 표본 크기도 더 커져야 합니다.

</br>

## #7
### SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요?

SVM을 비선형 분류 모델로 사용하기 위해 **저차원 공간의 데이터를 고차원 공간으로 매핑**하여 선형 분리가 가능한 데이터로 변환하여 처리합니다. SVM이 좋은 이유는 SVM은 데이터들을 선형 분리하며 최대 마진의 초평면을 찾는 크게 복잡하지 않은 구조이며, **커널 트릭을 이용**해 차원을 늘리면서 비선형 데이터들에도 좋은 결과를 얻을 수 있습니다. 또한 이진 분류 뿐만 아니라 수치 예측에도 사용될 수 있습니다. **Overfitting 경향이 낮으며** 노이즈 데이터에도 크게 영향을 받지 않습니다.

> SVM 회귀의 경우 SVM 분류와 반대로, 제한된 마진 오류(즉, 도로 밖의 샘플) 안에서 마진 안에 가능한 한 많은 샘플이 들어가도록 학습합니다.

</br>

## #8
###  ROC 커브에 대해 설명해주실 수 있으신가요?

ROC 커브는 거짓 긍정(False Positive)을 피하면서, 참 긍정(True Positive)을 탐지하는 것 사이의 트레이드오프를 관찰하기 위한 지표로 FPR(False Positive Rate)를 x축으로, TPR(True Positive Rate)를 y축으로 나타내는 곡선입니다.  
이러한 ROC curve는 왼쪽 상단에 가까울수록, 그래프의 아래 면적을 의미하는 AUC (Area Under the Curve)는 1에 가까울수록 좋은 성능이라고 판단합니다.

</br>

## #9
### 회귀 / 분류시 알맞은 손실함수와 이에 대한 설명

회귀 문제에서는 **MSE나 MAE** 손실함수를 사용합니다.  MSE는 예측값과 실제값 간의 차이를 제곱한 값의 평균으로, 회귀 문제에서 가장 많이 사용되는 손실함수입니다. **MAE는** 예측값과 실제값 간의 차이의 절댓값의 평균으로, **이상치(outlier)가 있는 데이터에 민감하지 않다**는 특징이 있습니다.  
분류 문제에서는 이진 분류의 경우 Binary **Cross Entropy**를 다중 분류의 경우 Categorical Cross Entropy를 주로 사용합니다. 이외에도 불균형 데이터셋의 성능을 향상시키기 위해 사용되는 Focal loss가 있습니다. Focal loss 는 Categorical Cross Entropy에 (1 - y_pred)^γ를 곱하여  γ (gamma)값이 증가할수록 쉬운 샘플에 대한 가중치가 줄어들어 어려운 샘플에 대해 더 집중적으로 학습할 수 있게 합니다.

> 손실함수(loss funciton)는 딥러닝 모델이 실제 레이블과 가장 가까운 값이 예측되도록 훈련할 때, 모델의 예측값과 실제 레이블 간의 거리를 측정하기 위해 사용되는 함수입니다.

</br>

## #10
### 최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?

**뉴턴법은 현재 x값에서 접선이 x축과 만나는 지점으로 x를 이동시켜 가면서 점진적으로 해를 찾아가는 방법**입니다. 초기값을 잘 주면 수렴 속도가 매우 빠르지만, 잘못 주면 시간이 오래 걸리거나 해를 찾지 못할 수 있습니다. 함수가 미분 가능해야하고 미분값이 0이 지점이 없어야합니다.   
반면 **Gradient Descent는 현재 위치에서 함수의 기울기를 이용하여 극소점을 찾아가는 방법**으로 local minimum에 빠질 수 있다는 문제가 있지만, 모든 차원과 모든 공간에서 적용이 가능합니다.  
Gradient Descent는 매 단계에서 함수의 기울기를 계산해야 하므로 계산 비용이 낮지만, 수렴 속도가 상대적으로 느릴 수 있습니다. Newton's Method는 매 단계에서 이차 도함수를 계산해야 하므로 계산 비용이 높아질 수 있지만, 수렴 속도가 빠를 수 있습니다.


</br>

## #11
### 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?

**머신러닝**은 모델에 대한 정교한 가정보다는 데이터의 다양한 피쳐를 사용하여 **높은 예측률**을 달성하고자 합니다. 따라서 머신러닝은 데이터의 복잡한 패턴을 인식하고 이를 통해 예측하는 능력이 뛰어나며, 데이터가 매우 큰 경우에도 높은 정확도로 예측이 가능합니다.  
반면 **통계적 접근방법**은 데이터의 분포와 가정을 통해 **신뢰 가능한 모델**을 만드는게 목적으로 다양한 통계 모델링 기법을 사용합니다. 통계적 접근방법은 데이터의 특성을 파악하고, 변수의 선택과 추정, 가설 검정 등을 수행하며 **어떤 피쳐가 어떤 원인을 주는지 알 수 있다**는 특징이 있습니다. 

</br>

## #12 
### Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?

딥러닝에서 가중치를 잘 초기화하는 것은 기울기 소실이나 local minima 등의 문제를 방지 가능합니다.  
**LeCun** 초기화은 **입력 노드 수를 고려**한 정규 분포와 균등 분포를 따르는 방법입니다.  
**Xavier** 초기화는 **입력 노드의 수와 출력 노드 수를 고려**하여 가중치를 초기화하는 방법입니다. Sigmoid 나 tanh 함수와는 좋은 결과를 보여주지만 ReLU 함수와 사용할 경우 0에 수렴하는 문제가 발생하는 초기화 방법입니다.  
**He** 초기화는 ReLU 와 함께 많이 사용되는 방법으로, LeCun 방법과 같이 **입력 노드의 수만을 고려하지만 상수 부분은 Xavier 초기화의 방법을 사용**합니다.

</br>

## #13
### 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)

**Sigmoid**는 **입력을 0 ~ 1 사이로 mapping**합니다. 하지만 입력값이 커질수록 미분값이 0에 수렴해 **gradient vanishing** 문제가 생기며 **zero-centered하지 않아** 학습이 느려집니다.  
**Tanh**는 sigmoid를 변형한 쌍곡선함수로, **입력을 -1 ~ 1 사이로 mapping해 zero-center 문제는 해결**했지만 gradient vanishing 문제는 해결하지 못했습니다.  
**ReLU**는 입력이 양수일 경우 **saturate 문제가 해결**되며, 단순히 max 함수를 사용해 **속도가 빠릅니다**. 입력이 음수일 경우에도 saturate 문제를 해결한 함수로는 **LeakyReLU**가 있습니다.  
분류 문제에는 **softmax**를 사용하기도 합니다.


</br>

## #14
### 효율적인 GPU의 사용을 위해 어떻게 dataloader를 조절할 수 있는가?

PyTorch의 DataLoader는 **`batch_size`** 로 batch의 크기를 조절하여 데이터를 미니 배치로 분할하고, 이들을 병렬적으로 로딩하고 전처리하는 함수로 데이터를 GPU로 로드할 수 있으며, 이를 통해 모델 학습 시간을 단축할 수 있습니다.  
DataLoader의 **`num_workers`** 하이퍼파라미터 값이 높을수록 DataLoader는 더 많은 프로세스를 생성하여 데이터 로딩 및 전처리를 병렬로 처리합니다. 이때, 각 워커는 CPU를 사용하여 데이터를 로드하고 전처리합니다. 이를 GPU 메모리로 이동시킬 때, **`pin_memory`** 파라미터를 사용하여 CPU 메모리와 GPU 메모리 간 데이터 이동을 최적화할 수 있습니다. **`pin_memory=True`** 로 설정하면 DataLoader는 Tensor를 CUDA 고정 메모리에 복사하여 GPU 메모리로 전송합니다.

</br>

## #15
### 하이퍼 파라미터는 무엇인가요?

하이퍼 파라미터(Hyper-parameter)는 모델링할 때, **사용자가 직접 세팅해주는 값**으로 learning rate, epoch나 SVM에서의 C, sigma 값, KNN에서의 K값 등이 있습니다.
하이퍼 파라미터는 정해진 최적의 값이 없으며, 사용자의 **선험적 지식을 기반으로 설정(휴리스틱)** 합니다. 이러한 하이퍼 파라미터 튜닝 기법에는 Manual Search, Grid Search, Random Search, Bayesian Optimization 등이 있습니다.

</br>

## #16
### 미니배치를 작게 할때의 장단점은?

전체 데이터를 쪼개서 여러 번 학습하기 때문에, 전체 training 데이터 셋을 배치로 사용것보다 계산량이 적어(즉 메모리 사용량이 적어), **학습 속도가 빠르다**는 장점이 있습니다. 반면에 big batch보다 느리고 loss function의 최솟값을 찾기 위해 자주 step의 방향을 바꿔야 하므로 **학습이 불안정**한 단점이 있습니다.

> 하지만 요즘엔 Adam optimizer나 batch normalization(BN)등의 기법을 사용함으로써 학습안정화가 정말 잘 되는 네트워크가 많습니다. 따라서 batch size가 커도 local minimum을 잘 지나쳐 global minimum으로 수렴할 수 있게되었습니다. 따라서 최근에는 batch size를 줄 수 있는만큼 최대한 크게줘야 좋다고 할 수 있습니다. batch size가 클수록 BN을 더욱 정확하게 계산할 수 있어 BN의 효과를 더욱 잘 누릴 수 있게되고 그렇게 되면 학습속도가 빨라지며 learning rate의 hyper parameter 설정에서 꽤나 자유로워질 수 있게됩니다.


</br>

## #17
### 캐시의 지역성에 대해 설명해주세요.

캐시 메모리는 적중률(Hit rate)을 극대화하기 위해 데이터 **지역성(Locality)** 의 원리를 사용합니다. 지역성(Locality)이란 기억 장치 내의 정보를 균일하게 액세스하는 것이 아닌 **특정 부분을 집중적으로 참조하는 특성**으로, 프로그램이 소규모의 특정 데이터 및 명령어 집합에 반복적으로 액세스하는 경향이 있다는 사실을 의미합니다.  
최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성인 **시간 지역성**과 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성인 **공간 지역성**이 있습니다.

> 캐시 메모리는 CPU의 처리 속도와 메모리의 속도 차이로 인한 병목현상을 완화하기 위해 사용하는 고속 버퍼 메모리입니다. 주기억장치에 있는 데이터를 액세스하려면 비교적 오랜 시간이 걸리게 되는데 이를 줄이기 위해 데이터를 빠르게 액세스할 수 있도록 중간에 캐시 메모리를 사용합니다. 따라서 데이터 요청이 들어오면, 원본 데이터가 담긴 곳에 접근하기 전에 먼저 캐시 내부부터 찾는데, 이때 원하는 데이터가 캐시에 있는 경우에 대한 비율을 캐시 메모리 적중률이라고 합니다.

---

# 인성
## #18
### 비 IT 동료와 효과적으로 의사소통하려면 어떻게 할지 말해주세요?

- 특정 IT 기술에 대한 직접적인 언급보다는 그 외의 부분에 대해 구체화하여 소통할 것 같습니다. 예를 들어 같이 진행하는 프로젝트에서 기술적인 문제가 있다고 하면, 먼저 해당 프로젝트에서 현재 문제사항이 무엇인지, 이게 왜 문제인지 사용자의 입장에서 구체적인 예시를 들며 납득할 수 있게 설명할 것 같습니다. 또한, 이를 해결하기 위한 기술적인 방법을 말하기 보다는 해당 문제를 해결하고 난 후, 수정 적용 결과에 대해 구체적으로 말하며 비 IT 직군의 입장에서 왜 현재 수정이 필요한지 이해하고, 수정이 되고 나면 자신은 어떤 식으로 다시 해당 프로젝트를 재구성해야할지 계획할 수 있도록 전달하고자 할 것 같습니다.

-  전문용어나 지식이 필요한 대화일 경우 쉽게 풀어 설명하거나 이에 대한 정보를 먼저 제공 -> 구체적 경험 풀어서 

- 최대한 전문용어는 빼고 결과 중심으로 명확히 의사전달을 할 것입니다. 또한, 자료를 쓸 수 있다면 분석과 같은 시각적 자료를 통해 결과물이나 요청사항 등으로 이해시킬 것입니다. + (개인적 경험)

</br>

## #19
### 이상적인 개발 환경이란 무엇이라고 생각하시나요?

- 활발한 코드리뷰, 피드백이 가능한 수평적 분위기 + 자유롭게 의견을 낼 수 있는 분위기 --> 성장 가능 + 프로그램 개선


- 이상적인 개발 환경은 협업하기 좋은 환경이라고 생각합니다. 이때, 제가 생각하는 협업하기 좋은 환경은 서로 직군이나 직급에 관계없이 진행하고 있는 프로젝트에 대한 의견을 공유할 수 있는 문화가 형성되어 있는 것입니다. 이러한 환경이 구성되어 있을 때, 각자의 업무에 집중하며 시너지를 낼 수 있다고 생각합니다.


- 제가 생각하는 이상적인 개발 환경이란 같이 개발지식을 나눌 동료들이 있고 피드백을 해줄 시니어들이 있는 것입니다. 주변 환경을 통해 성장하고 ~

</br>

## #20
### 상사와 의견차가 좁혀지지 않는 갈등이 일어났을 때 어떻게 해결할 것인가?

- 상사는 나보다 경험이 많음 -> 일단 믿고 수용, 하지만 나도 좋은 의견이 있을 경우엔 생각을 밝히고 의견 구함

- 먼저 상사님의 의견에 대해 더 들어보고자 할 것 같습니다. 그럼에도 제 생각에 맞지 않다고 생각되는 부분이 있는 경우, 합리적인 근거를 들며 설득하고자 할 것 같습니다. 만약 그런 상황에서 상사님이 저의 근거를 반박하게 되면 합리적인 이유가 있으면 그것대로, 그렇지 않더라도 상사님의 의견이기에 이에 수긍할 것 같습니다.


- 먼저 상사의 의견에 질문을 많이 하여 의견이 타당한 이유를 찾을 것 같습니다. 무조건적인 수용보다 타당한 이유를 알고 의견을 받아들인다면 더 일의 효율이 높아지기 때문입니다. 

</br>

## #21
### 어떤 개발자가 잘하는 개발자라고 생각하는가?

- 문제 인지 -> 해결책 제시 -> 프로세스 그림 -> 언어와 프레임워크로 구현 


- 함께 성장하기 위해 노력하는 사람이라고 생각합니다. 팀원들과 이야기하고 논의하다보면 서로 발전하게 된다고 생각합니다. 팀원들에게 공유하고, 코드 리뷰하며 열린 자세로 논의할 수 있는 개발자가 잘하는 개발자라고 생각합니다.


- 먼저 일을 잘한다라는 것에 대해 말해보고 싶습니다. 저는 일을 잘한다는 것은 본인이 맡은 업무에 대해 해내며, 이 과정에서 발생할 수 있는 생각지 못한 예외사항들도 잘 처리해나가는 것이라고 생각합니다. 때문에, 잘하는 개발자도 맡은 업무에 대해 해내며, 이 과정에서 문제가 발생하더라도 트러블 슈팅을 하며 유연하게 해결해나가는 개발자라고 생각합니다.

</br>

## #22
### 간단한 1분 자기소개 부탁드려요

- 이름 -> 직무와 관련된 경험 + 역량 -> 포부
외운 티 내지 않기


- 안녕하십니까! ㅇ 직무에 기여하고 싶은 ㅇ입니다. 저는 ㅇ직무에 적합한 저의 경험으로 저를 소개 드리겠습니다.
첫번째 저는 A/B Test를 통해 모델의 성능을 확인한 경험이 있습니다. 구현한 추천모델과 인기도 기반 모델을 중심으로 A/B Test를 진행했습니다. 이를통해 상품 추천 다양성 만족도 부분에서 추천 모델이 인기도 기반 모델보다 20% 높은 만족도를 얻은 것을 확인 할 수 있었습니다.
두번째 저는 ... 
제가 경험한 역량을 바탕으로 필요한 인재가 될 수 있도록 노력하겠습니다. 감사합니다.


나의 특징 / 성격/ 가치관/ 주변의 평/ 성격 장단점 / 특이경험/ 성공경험
나의 경험 (상황 / 결과)
진정성 강조 (내가 어떻게 이렇게 할 수 있었는지)
포부 (이를 어떻게 직무에 적용해나갈 것인지)

</br>

## #23
### 최근 관심있는 분야가 있으신가요?

- 기업분석... 지원한 기업과 연관..?

- 최근에는 멀티 모달에 관심이 있습니다. 특히 chat gpt가 나온 이후로는 VQA(Visual Question Answering)에 더 관심이 생기게 되었습니다. 아직 이미지로 복잡한 문서를 분석하여 처리하는 부분에 있어 어려움이 있는 것으로 알고 있습니다. chat gpt는 현재 텍스트로 질문하면 구체적인 질의응답을 할 수 있습니다. 이렇게 특정 이미지에 대한 질의응답도 가능해지면 더욱 유용할 것이라고 생각됩니다. 또한, 앞으로 인공지능 분야에서 점점 다양한 종류의 데이터 타입을 학습하는 멀티 모달 모델들이 나올 것이라고 예상되어 관심이 가는 분야입니다.

- 머신러닝 엔지니어에게 모델 개발뿐 아니라 데이터를 검증하고 전처리하여 모델을 재학습시키는 과정인 머신러닝 파이프라인도 중요하다고 생각합니다. 저는 docker를 최근에 공부하기 시작하면서 모델의 성능을 재현하고 배포하는 단계를 이해하려고 노력하고 있습니다.