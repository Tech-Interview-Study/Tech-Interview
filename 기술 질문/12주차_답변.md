# 기술
## Operating System
- [가상 메모리에 대해 설명해주세요.](#1)
## Deep Learning
- [Local Minima와 Global Minima에 대해 설명해주세요.](#2)
- [GAN에서 Generator 쪽에도 BN을 적용해도 되는지?](#3)

## #1
### 가상 메모리에 대해 설명해주세요.
메모리의 물리적 공간의 제약을 극복하기 위해 나온 기법으로, 프로그램의 전체 내용을 메모리에 올리는 것이 아니라 프로그램을 페이지 단위로 쪼개고, 필요한 페이지만 메모리에 올려서 더욱 많은 프로그램을 동시에 메모리에 올릴 수 있게 하는 것입니다. 
> 페이지 단위일 수도 있고, 세그먼트 단위일 수도 있지만 **현재 대부분은 페이지 단위를 사용합니다**.

> **메모리 관리 장치(MMU)는 가상 주소를 이용해 실제 데이터가 담겨 있는 주소로 변환**해 줍니다. 이때 가상 주소 공간은 하나의 프로세스가 메모리에 저장되는 논리적인 모습을 가상 메모리에 구현한 공간이며, 가상 주소는 해당 공간을 가리키는 주소입니다.

> 프로세스가 필요로 하는 모든 페이지가 주기억장치에 적재될 수 없는 경우, 일부 페이지는 하드 디스크 등의 보조기억장치에 저장되어 있어야 합니다. 만약 페이지가 주기억장치에 없으면, 페이지 폴트(Page Fault)가 발생하여, 운영체제는 보조 기억장치에서 해당 페이지를 찾아와 주기억장치에 올리고, 다시 해당 프로세스를 실행합니다.  
하지만, 만약 물리 메모리가 모두 사용중인 상황이라면, **페이지 교체 알고리즘**을 통해 물리 메모리의 한계를 극복하게 됩니다.
https://github.com/Tech-Interview-Study/Tech-Interview/blob/main/%EA%B8%B0%EC%88%A0%20%EC%A7%88%EB%AC%B8/06%EC%A3%BC%EC%B0%A8_%EB%8B%B5%EB%B3%80.md#15

## #2
### Local Minima와 Global Minima에 대해 설명해주세요.
Global Minimum은 loss function에서 loss값이 가장 작은 지점이며 이 지점에 도달하는 것이 딥러닝의 학습 최종 목표입니다. 
Local Minimum은 Global Minimum에 다다르지 않고, 기울기가 0이 되는 다른 지점을 말합니다.
local minima에 빠지지 않기 위해서 모멘텀, adaptive learning rate등과 같은 옵티마이저 기법을 사용할 수 있습니다.

> local minima 문제에도 불구하고 딥러닝이 잘되는 이유?
local minima가 사실은 고차원(High Dimensional)의 공간에서는 발생하기 힘든, 매우 희귀한 경우이기 때문입니다. 실제 딥러닝 모델에서는 weight가 수도없이 많으며, 그 수많은 weight가 모두 local minima에 빠져야 weight update가 정지되기 때문에 local minima는 큰 문제가 되지 않습니다.

## #3
### GAN에서 Generator 쪽에도 BN을 적용해도 되는지?
Batch Normalization은 데이터의 평균과 분산을 정규화하여 학습을 안정화시키는데, GAN의 Generator에서는 이러한 안정성을 유지하는 것이 mode collapse를 심화시킬 수 있습니다. Generator는 노이즈로부터 실제같은 데이터를 생성하는 역할을 수행하는데, BN의 적용으로 인해 생성된 데이터의 다양성과 품질이 저하될 수 있습니다. 
반면, Discriminator에 Batch Normalization을 적용하는 것은 일반적으로 GAN의 안정성 및 성능 향상에 도움이 될 수 있습니다. (Generator의 다양한 레이블에 대해 실제같은 이미지를 생성하는 것을 도움 ; mode collapse 문제 완화)  
\* [mode collapse 문제](http://dl-ai.blogspot.com/2017/08/gan-problems.html)  
MNIST 데이터세트를 이용하여 GAN을 학습시키다 보면 같은 숫자만 계속해서 생성되는 현상을 볼 수 있는데, 이것이 바로 `mode collapsing`이 발생한 것입니다. **생성자 입장에서는 어떤 숫자를 만들든 판별자만 속이면 되기때문에 게임의 목적은 달성했다고 말할 수 있고, 판별자 입장에서도 잘못된 판단이라고 말할 수 없습니다.**


> DCGAN 모델에서는 학습의 안정성을 위해 BN을 생성자의 output layer, 판별자의 input layer 제외하고 적용함. 이는 변형을 가하지 않은 데이터를 판별자에 전달함으로써 판별자가 더욱 정확한 분포를 파악할 수 있도록 하는 것이라 판단됍니다. 하지만 GAN의 종류에 따라 BN이 잘 작동할 수도 아닐 수도 있습니다.
